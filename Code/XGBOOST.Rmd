```{r}
#standard_data<- write.csv(standard_data,"D:/study stuff/UIC/IDS 572 Data Mining/Home #Work/HW4 Case study/IMB 623 VMWare- Digital Buyer Journey/standard_data.csv")
standard_data<- read.csv("D:/study stuff/UIC/IDS 572 Data Mining/Home Work/HW4 Case study/IMB 623 VMWare- Digital Buyer Journey/standard_data.csv")
standard_data <- standard_data[,-1]
#write.csv(target,"D:/study stuff/UIC/IDS 572 Data Mining/Home Work/HW4 Case study/IMB 623 #VMWare- Digital Buyer Journey/target.csv")
target<- read.csv("D:/study stuff/UIC/IDS 572 Data Mining/Home Work/HW4 Case study/IMB 623 VMWare- Digital Buyer Journey/target.csv")
target <- target[,-1]
#library(plyr)
#library(dplyr) 
#library(LiblineaR)
# tryTypes <- c(6)
# tryCosts <- c(0.1)#c(1,0.8,0.5,0.2,0.1,0.09,0.01,0.001)
# bestCost <- NA
# bestRecall <- 0
# bestAcc <- 0
# bestType <- NA
# sum_indx <- c()
# recall_list <- c()
# folds <- split(standard_data, cut(sample(1:nrow(standard_data)),2))
# 
# for(ty in tryTypes){
#     for(co in tryCosts){
#       for (k in 1:length(folds)) {
#         eval <- ldply(folds[k], data.frame)
#         evalu <- eval[,-c(1,2)]
#         tra <- ldply(folds[-k], data.frame)
#         trai <- tra[,-c(1,2)]
#         model.Liblinear <- LiblineaR(data=standard_data, target=target, type=ty, cost=co, #verbose=FALSE)
#         pred_log <- predict(model.Liblinear,standard_data)
#         confmax <-table(target,unlist(pred_log))
#         recall_list[k] <-  mean(diag(confmax)/colSums(confmax)) 
#       }
#       cat("Results for C=",co," :  recall = ",mean(recall_list), " \n",sep="")
#       if(mean(recall_list)>bestRecall){
#         bestCost <- co
#         bestRecll <- mean(recall_list)
#         bestAcc <- pred_log
#         bestType <- ty
#       }
#       sum_indx <- c()
#       r<-0
#         for (i in 1:ncol(model.Liblinear$W)) {
#           if (sum( model.Liblinear$W[,i])<0) {
#             r<- r+1
#             sum_indx[r] <- i
#           }
#         }
#       cat("No. of significant variables for C=",co," is : #",length(sum_indx),".\n",sep="")
#       }
#   }
```





Using only the final selected significant variables, we build the RandomForest Model models:
```{r}
write.csv(sig_var_names,"D:/study stuff/UIC/IDS 572 Data Mining/Home Work/HW4 Case study/IMB 623 VMWare- Digital Buyer Journey/sig_var_names.csv")
sig_var_names<- sig_var_names[,-1]#colnames(standard_data[sum_indx])
for (i in 1:length(sig_var_names)) {
sig_var_names[i]<-strsplit(sig_var_names[i], "[.]")[[1]][1]  
print(sig_var_names[i])
}
#sig_var_names
unique(sig_var_names)
```


```{r}
unique(sig_var_names)
tgt <- sd1$target
b  <- cbind(tgt,sd2[unique(sig_var_names)])
mtry<- c(floor(sqrt(ncol(b))))
 rf.fit = randomForest(b$tgt~., data = b, mtry=5, ntree = 30)
 
 pred.rf <-predict(rf.fit,b[-1])
 confmax<-table(rf.fit$predicted,pred.rf) 
 mean(diag(confmax)/colSums(confmax))
 
 diag(confmax)/colSums(confmax)
 
```

Cleaning Validation data to check the model performance:

```{r}
for (i in 1:ncol(dt.num)){
  
  #Imputing the NA's with the Median Values
  i[is.na(i)] = median(i, na.rm=TRUE)
}
```




Now lets run an Exteam Gradient Boosting model.

```{r}
library(xgboost)
label <- as.integer(target)-1
n = nrow(standard_data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(standard_data[train.index,])
train.label = label[train.index]
test.data = as.matrix(standard_data[-train.index,])
test.label = label[-train.index]
xgb.train=xgb.DMatrix(data=train.data,label=train.label)
xgb.test=xgb.DMatrix(data=test.data,label=test.label)
```

Now we define the parameters, lets use defult values first.

```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.3,max_depth=6,gamma=0,subsample=1,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
We can now use the inbuilt xgb.cv function for cross validation to find the best nrounds for this model.

```{r}
xgb_cv1= xgb.cv(params=params,data=xgb.train,nrounds=200,nfold=10,showsd=T,stratified=T,print_every_n=10,early_stopping_rounds=10, maximize = F)
xgb_cv1
```
Mean CV test error =
```{r}
cve=data.frame(xgb_cv1$evaluation_log)
m=min(cve$test_mlogloss_mean)
m
```
The bset itration is:
```{r}
i=cve[cve$test_mlogloss_mean==m,1]
i
i=i+10
```


Now lets build the extream gradient boosting model with the best n rounds obtained from Cross validation 
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=i,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
increasing the n rounds to see if it still gives the least mlogloss error within the nrounds obtained through cross validation.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=50,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```


As we can see the test mlogloss error is decreasing with every itration and at some point starts to increase, so the best itration with the least error can be found from this.

Training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
k=e[e$val2_mlogloss==m,]
k
```
So through cross validation we can choose the best nrounds, use it to optimize the XGB model and find the least test mlogloss error running least possible number of itrations (here we can use best nrounds +10 to mae sure it captures the least value) .the we use different parameters to  get  the  best accuracy possible .

varable importance is as follows.

```{r}
imprt=xgb.importance(colnames(train.data),model=xgb.fit)
print(imprt)
```
plot of importance 
```{r}
xgb.plot.importance(importance_matrix=imprt[1:100])
```

Predictions with test data.
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
xgb.pred
pred=xgb.pred
```
 The highest probability is taken as the prediction and passed to a new column, Prediction.
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
levels(target)
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```    
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```

Confusion matrix and Recall.

```{r}
#pred_m=matrix(xgb.pred,nrow=nclass,ncol=length(pred/nclass))%>%
 # t()%>%
  #data.frame()%>%
  #mutate(label_t=xgb.pred$label,max_prob=xgb.pred$prediction)
#confution matrix
#table(Prediction=pred_m$max_prob,Actual=pred_m$label_t)
A<-table(pred=xgb.pred$prediction[],actual=xgb.pred$label)
#diag(A)/colSums(A)
#for (i in 1:6) {
#  X<- mean(diag(A[i])/colSums(A[i]))
#  print(X)
#}
```


########################################################################################

Now lets try out different parameters and see.
We are reducing the eta and max depth and other parameters to see how it affects the model. 
```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.05,max_depth=3,gamma=3,subsample=0.75,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
Now lets build the extream gradient boosting model for the new parameters.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=i,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
k=e[e$val2_mlogloss==m,]
k
```
Predict with test
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
#xgb.pred
pred=xgb.pred
```
  The highest probability is taken as the prediction and passed to a new column, Prediction.
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```
Now lets see the same parameters for except for depth, lets increse the depth and see how it affects the model.
```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.05,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
Now lets build the extream gradient boosting model for the new parameters.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=i,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
e[e$val2_mlogloss==m,]
```
Predict with test
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
#xgb.pred
pred=xgb.pred
```
 Use the predicted label with the highest probability
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```

Lets use lower eta and higher depth and see what changes.

```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.01,max_depth=6,gamma=3,subsample=0.75,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
Now lets build the extream gradient boosting model for the new parameters.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=i,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
e[e$val2_mlogloss==m,]
```
Predict with test
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
#xgb.pred
pred=xgb.pred
```
 Use the predicted label with the highest probability
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```
Now lets see the same parameters for except for depth, lets increse the depth and see how it affects the model.
```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.05,max_depth=6,gamma=3,subsample=0.75,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
Now lets build the extream gradient boosting model for the new parameters.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=i,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
e[e$val2_mlogloss==m,]
```
Predict with test
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
#xgb.pred
pred=xgb.pred
```
 Use the predicted label with the highest probability
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```

We find that the n rounds is not properly capturng the mower errors so lets try 100 itrations for the same parameters as above and see the improvement in accuracy.

```{r}
nclass = length(levels(target))
params = list(booster="gbtree",eta=0.01,max_depth=6,gamma=3,subsample=0.75,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)
```
Now lets build the extream gradient boosting model for the new parameters.
```{r}
xgb.fit=xgb.train(params=params,data=xgb.train,
  nrounds=100,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
  
)
xgb.fit
```
training and test error plot. we can compare test and train error and improve the model
```{r}
e=data.frame(xgb.fit$evaluation_log)
plot(e$iter,e$val1_mlogloss,col= "red")
lines(e$iter,e$val2_mlogloss,col= "blue")
```
Minium value for the test error and the itration giving the minimum value can be found by,
```{r}
m=min(e$val2_mlogloss)
m
e[e$val2_mlogloss==m,]
```
Predict with test
```{r}
xgb.pred = predict(xgb.fit,test.data,reshape=T)
#xgb.pred
head(xgb.pred)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
#xgb.pred
pred=xgb.pred
```
 Use the predicted label with the highest probability
```{r}
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred
```
accuracy
```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```
[1] "Final Accuracy = 94.25%"
We initially used the parameters these params = list(booster="gbtree",eta=0.3,max_depth=6,gamma=0,subsample=1,colsample_bytree=1, objective="multi:softprob", eval_metric="mlogloss", num_class=nclass)

Through cross-validation we get the best nrounds and use that nrounds to compare the different models obtained using different parameters.

The first model used the parameters above and got an accuracy of 945.26%, with the min mlogloss coming to 0.1231.

Now we run the second XGboosted model using Eta= 0.05 max depth = 3 ,  gamma =3 , subsample=0.75,colsample_bytree=1,
we get an accuracy of 94.21 which is not very less from before but the mlogloss is 0.282552 for the same number of iterations.
Keeping all other parameters the same we can increase the max depth = 5, here also the accuracy decreases 94.04% but we get a better mlogloss = 0.2688 for the same number of iterations.
